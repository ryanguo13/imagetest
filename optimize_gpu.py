#!/usr/bin/env python3
"""
GPUä¼˜åŒ–è„šæœ¬ - ä¸€é”®ä¼˜åŒ–ä½ çš„è®­ç»ƒé…ç½®ä»¥æ¦¨å¹²GPUæ€§èƒ½

ä½¿ç”¨æ–¹æ³•:
    python optimize_gpu.py --config configs/lightweight_config.yaml
    
åŠŸèƒ½:
    1. è‡ªåŠ¨æ£€æµ‹GPUè§„æ ¼
    2. ç”Ÿæˆä¼˜åŒ–é…ç½®
    3. å¯é€‰ï¼šè‡ªåŠ¨æœç´¢æœ€ä¼˜batch size
    4. å®æ—¶GPUç›‘æ§
"""

import argparse
import os
import sys
import torch
import yaml
from pathlib import Path

# Add project root to path
sys.path.append(str(Path(__file__).parent))

try:
    from utils.gpu_optimizer import auto_optimize_config, GPUMonitor, BatchSizeOptimizer
    from src.main import create_model
    GPU_OPTIMIZER_AVAILABLE = True
except ImportError as e:
    print(f"Warning: GPU optimizer import failed: {e}")
    print("Will provide basic optimization without advanced monitoring.")
    GPU_OPTIMIZER_AVAILABLE = False

def print_gpu_info():
    """æ‰“å°GPUä¿¡æ¯"""
    print("ğŸš€ GPUä¼˜åŒ–å·¥å…·")
    print("=" * 50)
    
    if not torch.cuda.is_available():
        print("âŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡")
        print("å»ºè®®: è¯·ç¡®ä¿å®‰è£…äº†æ­£ç¡®çš„PyTorch GPUç‰ˆæœ¬")
        return False
    
    print(f"âœ… æ£€æµ‹åˆ°GPU: {torch.cuda.get_device_name(0)}")
    print(f"ğŸ“Š æ˜¾å­˜å®¹é‡: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    print(f"ğŸ”§ CUDAç‰ˆæœ¬: {torch.version.cuda}")
    print(f"âš¡ PyTorchç‰ˆæœ¬: {torch.__version__}")
    print()
    
    return True

def create_basic_optimized_config(base_config_path: str, output_path: str):
    """åˆ›å»ºåŸºç¡€ä¼˜åŒ–é…ç½®ï¼ˆä¸ä¾èµ–GPUæ£€æµ‹ï¼‰"""
    with open(base_config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åŸºç¡€ä¼˜åŒ–ï¼šä¸ä¾èµ–GPUæ£€æµ‹
    config.update({
        'batch_size': max(config.get('batch_size', 16) * 2, 32),  # è‡³å°‘ç¿»å€
        'base_channels': max(config.get('base_channels', 32) * 2, 64),  # è‡³å°‘ç¿»å€
        'num_blocks': max(config.get('num_blocks', 4) * 2, 8),  # è‡³å°‘ç¿»å€
        'num_workers': 8,  # å¢åŠ æ•°æ®åŠ è½½å¹¶è¡Œåº¦
        'patch_size': max(config.get('patch_size', 128), 192),  # å¢å¤§patch
        'use_mixed_precision': True,
        'use_perceptual': True,
        'pin_memory': True,
        'persistent_workers': True,
        'prefetch_factor': 4,
        'grad_clip': 1.0,
    })
    
    with open(output_path, 'w') as f:
        yaml.dump(config, f, default_flow_style=False)
    
    return output_path

def optimize_config(base_config_path: str, output_path: str = None):
    """ä¼˜åŒ–é…ç½®æ–‡ä»¶"""
    if output_path is None:
        base_name = Path(base_config_path).stem
        output_path = f"configs/{base_name}_gpu_optimized.yaml"
    
    print("ğŸ”§ æ­£åœ¨ä¼˜åŒ–é…ç½®...")
    
    if GPU_OPTIMIZER_AVAILABLE:
        optimized_path = auto_optimize_config(base_config_path, output_path)
    else:
        print("ä½¿ç”¨åŸºç¡€ä¼˜åŒ–æ¨¡å¼ï¼ˆæ— GPUæ£€æµ‹ï¼‰...")
        optimized_path = create_basic_optimized_config(base_config_path, output_path)
    
    # æ˜¾ç¤ºä¼˜åŒ–å‰åå¯¹æ¯”
    with open(base_config_path, 'r') as f:
        original_config = yaml.safe_load(f)
    
    with open(optimized_path, 'r') as f:
        optimized_config = yaml.safe_load(f)
    
    print("\nğŸ“ˆ ä¼˜åŒ–å¯¹æ¯”:")
    print("-" * 50)
    improvements = [
        ('batch_size', 'Batch Size'),
        ('base_channels', 'Base Channels'), 
        ('num_blocks', 'Num Blocks'),
        ('num_workers', 'Num Workers'),
        ('patch_size', 'Patch Size'),
    ]
    
    for key, name in improvements:
        old_val = original_config.get(key, 'N/A')
        new_val = optimized_config.get(key, 'N/A')
        if old_val != new_val:
            print(f"{name:15}: {old_val:>6} â†’ {new_val:>6} {'ğŸš€' if new_val > old_val else 'ğŸ’¡'}")
    
    # æ–°å¢çš„ä¼˜åŒ–åŠŸèƒ½
    new_features = [
        ('use_mixed_precision', 'æ··åˆç²¾åº¦è®­ç»ƒ'),
        ('pin_memory', 'å†…å­˜å›ºå®š'),
        ('persistent_workers', 'æŒä¹…åŒ–Worker'),
        ('compile_model', 'æ¨¡å‹ç¼–è¯‘ä¼˜åŒ–'),
    ]
    
    print("\nâœ¨ æ–°å¢ä¼˜åŒ–åŠŸèƒ½:")
    print("-" * 50)
    for key, name in new_features:
        if optimized_config.get(key, False):
            print(f"âœ… {name}")
    
    return optimized_path

def benchmark_performance(config_path: str):
    """æ€§èƒ½åŸºå‡†æµ‹è¯•"""
    if not GPU_OPTIMIZER_AVAILABLE:
        print("\nâš ï¸ åŸºå‡†æµ‹è¯•éœ€è¦å®Œæ•´çš„GPUä¼˜åŒ–å™¨ï¼Œå½“å‰ä¸å¯ç”¨")
        print("å»ºè®®æ‰‹åŠ¨è°ƒæ•´batch_size: 32 â†’ 48 â†’ 64ï¼Œç›´åˆ°æ˜¾å­˜ä¸å¤Ÿä¸ºæ­¢")
        return config_path
    
    print("\nğŸƒ è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•...")
    
    with open(config_path, 'r') as f:
        config = yaml.safe_load(f)
    
    # åˆ›å»ºæ¨¡å‹ç”¨äºæµ‹è¯•
    model = create_model(config, stage=0)
    
    # åˆ›å»ºæ ·æœ¬æ•°æ®
    batch_size = config['batch_size']
    patch_size = config['patch_size']
    sample_data = torch.randn(batch_size * 2, 3, patch_size, patch_size)
    
    # è‡ªåŠ¨æœç´¢æœ€ä¼˜batch size
    optimizer = BatchSizeOptimizer(model, config)
    optimal_batch_size, stats = optimizer.find_optimal_batch_size(
        sample_data, target_gpu_util=85.0, max_batch_size=batch_size * 4
    )
    
    # æ›´æ–°é…ç½®æ–‡ä»¶
    if optimal_batch_size != batch_size:
        print(f"\nğŸ“ å»ºè®®æ›´æ–°batch_size: {batch_size} â†’ {optimal_batch_size}")
        config['batch_size'] = optimal_batch_size
        
        # ä¿å­˜æ›´æ–°çš„é…ç½®
        output_path = config_path.replace('.yaml', '_optimized.yaml')
        with open(output_path, 'w') as f:
            yaml.dump(config, f, default_flow_style=False)
        print(f"ğŸ’¾ å·²ä¿å­˜ä¼˜åŒ–é…ç½®åˆ°: {output_path}")
        return output_path
    
    return config_path

def monitor_training(duration: int = 60):
    """å®æ—¶ç›‘æ§è®­ç»ƒè¿‡ç¨‹ä¸­çš„GPUä½¿ç”¨æƒ…å†µ"""
    if not torch.cuda.is_available():
        print("\nâŒ æœªæ£€æµ‹åˆ°CUDAè®¾å¤‡ï¼Œæ— æ³•ç›‘æ§GPU")
        return
    
    if not GPU_OPTIMIZER_AVAILABLE:
        print(f"\nğŸ‘€ å¯åŠ¨åŸºç¡€GPUç›‘æ§ ({duration}ç§’)...")
        print("æç¤º: ç°åœ¨å¼€å§‹ä½ çš„è®­ç»ƒï¼Œæˆ‘ä¼šæ˜¾ç¤ºåŸºç¡€GPUä¿¡æ¯")
        print("æŒ‰ Ctrl+C æå‰ç»“æŸç›‘æ§")
        
        try:
            import time
            start_time = time.time()
            while time.time() - start_time < duration:
                memory_allocated = torch.cuda.memory_allocated() / 1024**3
                memory_total = torch.cuda.get_device_properties(0).total_memory / 1024**3
                memory_util = (memory_allocated / memory_total) * 100
                
                print(f"\rğŸ¯ æ˜¾å­˜ä½¿ç”¨: {memory_util:>3.0f}% ({memory_allocated:.1f}GB / {memory_total:.1f}GB)", end='')
                time.sleep(1)
        except KeyboardInterrupt:
            pass
        print("\n\nğŸ’¡ æç¤º: å¦‚æœæ˜¾å­˜ä½¿ç”¨ç‡ä½ï¼Œå¯ä»¥å¢å¤§batch_size")
        return
    
    print(f"\nğŸ‘€ å¯åŠ¨GPUç›‘æ§ ({duration}ç§’)...")
    print("æç¤º: ç°åœ¨å¼€å§‹ä½ çš„è®­ç»ƒï¼Œæˆ‘ä¼šç›‘æ§GPUä½¿ç”¨æƒ…å†µ")
    print("æŒ‰ Ctrl+C æå‰ç»“æŸç›‘æ§")
    
    monitor = GPUMonitor()
    monitor.start_monitoring(interval=1.0)
    
    try:
        import time
        start_time = time.time()
        while time.time() - start_time < duration:
            stats = monitor.get_current_stats()
            if stats:
                gpu_util = stats.get('gpu_utilization', 0)
                memory_util = stats.get('memory_utilization', 0)
                memory_used = stats.get('memory_allocated_gb', 0)
                
                print(f"\rğŸ¯ GPUåˆ©ç”¨ç‡: {gpu_util:>3.0f}% | "
                      f"æ˜¾å­˜ä½¿ç”¨: {memory_util:>3.0f}% ({memory_used:.1f}GB)", end='')
            
            time.sleep(1)
    except KeyboardInterrupt:
        pass
    finally:
        monitor.stop_monitoring()
        
        # æ˜¾ç¤ºæ€»ç»“
        avg_stats = monitor.get_average_stats()
        if avg_stats:
            print(f"\n\nğŸ“Š ç›‘æ§æ€»ç»“:")
            print(f"å¹³å‡GPUåˆ©ç”¨ç‡: {avg_stats.get('avg_gpu_util', 0):.1f}%")
            print(f"å¹³å‡æ˜¾å­˜ä½¿ç”¨: {avg_stats.get('avg_memory_util', 0):.1f}%")
            print(f"å³°å€¼æ˜¾å­˜ä½¿ç”¨: {avg_stats.get('peak_memory_gb', 0):.1f}GB")
            
            avg_gpu_util = avg_stats.get('avg_gpu_util', 0)
            if avg_gpu_util < 70:
                print("ğŸ’¡ å»ºè®®: GPUåˆ©ç”¨ç‡è¾ƒä½ï¼Œå¯ä»¥è€ƒè™‘å¢å¤§batch_sizeæˆ–æ¨¡å‹å¤æ‚åº¦")
            elif avg_gpu_util > 95:
                print("âš ï¸  è­¦å‘Š: GPUåˆ©ç”¨ç‡è¿‡é«˜ï¼Œå¯èƒ½å½±å“ç³»ç»Ÿç¨³å®šæ€§")
            else:
                print("âœ… GPUåˆ©ç”¨ç‡è‰¯å¥½!")

def main():
    parser = argparse.ArgumentParser(description='GPUä¼˜åŒ–å·¥å…· - æ¦¨å¹²ä½ çš„æ˜¾å¡æ€§èƒ½!')
    parser.add_argument('--config', type=str, default='configs/lightweight_config.yaml',
                       help='åŸºç¡€é…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--output', type=str, default=None,
                       help='è¾“å‡ºé…ç½®æ–‡ä»¶è·¯å¾„')
    parser.add_argument('--benchmark', action='store_true',
                       help='è¿è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--monitor', type=int, default=0,
                       help='ç›‘æ§GPUä½¿ç”¨æƒ…å†µ(ç§’æ•°ï¼Œ0è¡¨ç¤ºä¸ç›‘æ§)')
    parser.add_argument('--skip-optimization', action='store_true',
                       help='è·³è¿‡é…ç½®ä¼˜åŒ–ï¼Œä»…è¿è¡Œå…¶ä»–åŠŸèƒ½')
    
    args = parser.parse_args()
    
    # æ£€æŸ¥GPU
    if not print_gpu_info():
        return
    
    # æ£€æŸ¥é…ç½®æ–‡ä»¶
    if not os.path.exists(args.config):
        print(f"âŒ é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {args.config}")
        print("å¯ç”¨çš„é…ç½®æ–‡ä»¶:")
        config_dir = Path("configs")
        if config_dir.exists():
            for config_file in config_dir.glob("*.yaml"):
                print(f"  - {config_file}")
        return
    
    optimized_config_path = args.config
    
    # ä¼˜åŒ–é…ç½®
    if not args.skip_optimization:
        optimized_config_path = optimize_config(args.config, args.output)
        print(f"\nğŸ‰ é…ç½®ä¼˜åŒ–å®Œæˆ: {optimized_config_path}")
    
    # æ€§èƒ½åŸºå‡†æµ‹è¯•
    if args.benchmark:
        try:
            optimized_config_path = benchmark_performance(optimized_config_path)
        except Exception as e:
            print(f"âš ï¸  åŸºå‡†æµ‹è¯•å¤±è´¥: {e}")
    
    # GPUç›‘æ§
    if args.monitor > 0:
        monitor_training(args.monitor)
    
    print(f"\nğŸš€ ä½¿ç”¨ä¼˜åŒ–åçš„é…ç½®è®­ç»ƒ:")
    print(f"python src/main.py --config {optimized_config_path}")
    
    print(f"\nğŸ’¡ å…¶ä»–å»ºè®®:")
    print(f"1. è®­ç»ƒæ—¶è¿è¡Œ 'python optimize_gpu.py --monitor 300' æ¥ç›‘æ§GPUä½¿ç”¨æƒ…å†µ")
    print(f"2. å¦‚æœä»ç„¶åˆ©ç”¨ç‡ä¸é«˜ï¼Œå°è¯•å¢å¤§ batch_size æˆ– base_channels")
    print(f"3. ç¡®ä¿æ•°æ®åŠ è½½ä¸æ˜¯ç“¶é¢ˆï¼šå¢å¤§ num_workers")

if __name__ == "__main__":
    main() 